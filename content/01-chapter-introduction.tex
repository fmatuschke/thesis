\setcounter{chapter}{0}
\chapter{Introduction}
\label{sec:intro}
% 
One of the biggest unsolved questions in science today is how the brain, especially the human brain, functions.
An important role is played by the connectivity of neurons among each other.
They may be connected via nerve fibers (\ie{} axons, myelinated or not myelinated) over a relatively short distance to neighboring neurons or to more distant regions of the brain.
Understanding the brain's connectome, \ie{} the totality of connections, and the signal processing in the cells involved will allow us to explore and harness the origin of specialized abilities such as object recognition or memory.
Because the brain is such a large and complex structure relative to the size of its individual building blocks, neuroscientists are joining forces in collaborative projects such as the Human Brain Project to solve this problem together. \cite{Markram2006, Shen2012, Amunts2013, Amunts2016}
\par
% 
Techniques in machine learning, especially deep learning, profit from a better undestanding of the brain. There different types of neural networks are able to solve difficult problems that are almost impossible to solve with a classical algorithm.
A better understanding of the neural network as a subset of the connectome can therefore help to design artificial neural networks to efficiently solve tasks like image recognition, and vice versa. \cite{murphy2013machine, Goodfellow-et-al-2016}
\par
% 
The only technique currently able to measure the connectome of humans in the living brain, \ie{} invivo, is by using \ac{dMRI}.
\ac{dMRI} have a rather low resolution of about one millimeter relative to the structural configuration of nerve fiber bundles of several micrometers.
This low resolution can lead to misleading results due to the models and resolution limits involved (see ,meyer hein).
Therefore, a higher resolution of fiber pathways and the comprising individual fibers is required.
Such a dataset can also be used to learn from and improve the models applied to the lower resolution datasets, which can help in diagnosis. \cite{MaierHein2017, Schilling2021, Yendiki2021}
\par
% 
\ac{3D-PLI} is a microscopic imaging technique that allows the measurement of orientation of nerve fibers inside a brain section.
The myelin sorounding the axons of nerve fibers have a birefringence property, which changes the polarization state of light in relation to its orientation.
The change of the signal allows to analyse the underlying nerve fiber oritentation.
The brain slices are about $\SI{60}{\micro\meter}$ thick and the image resolution is in the order of a few micrometers.
This would yield a connectome for an entire human brain at unprecedented resolution. \cite{Axer2011a, Axer2011, Axer2016}
\par
% 
To improve the understanding of the underlying nerve fiber orientations, simulations play an important role.
Because the signal from an image pixel originates from a volume that contains multiple nerve fiber orientations, the resulting interpretation can be challenging.
Currently, there is no experimental model for nerve fibers that could serve as groundthruth.
For this reason, simulations are used.
They also help in the study of certain effects, as they allow full control over the experimental settings.
In addition, simulations can also generate a large amount of simulated experimental data sets.
For algorithms such as those used in machine learning, the amount of training data plays an important role.
Therefore, the simulation and generation of such datasets should be done as fast as possible to obtain a large training dataset.
\cite{Ginsburger2018, ginsburgerDis2019, Callaghan2019, Menzel2020}
\par
%
This dissertation provides a novel open-source software package \ac{fastPLI} whose main purpose is to provide a method for modeling dense, non-colliding 3D nerve fiber models.
These models are then used to simulate them in a virtual \ac{3D-PLI} experiment by calculating the effect of birefringence on polarized light using the M\"{u}ller-Stokes calulus. \cite{Matuschke2019, Matuschke2021, Reuter2019}\\
An important issue is the use of supercomputing architectures with efficiently developed algorithms to enable simulations of larger models and volumes.
The usefulness will be investigated and limitations will be demonstrated.